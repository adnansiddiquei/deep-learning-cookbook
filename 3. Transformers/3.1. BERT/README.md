# 3.1. Bidirectional Encoder Representations from Transformers (BERT)
This section aims to reproduce the "BERT: Pre-training of Deep Bidirectional
Transformers for Language Understanding" (2019) [1].
Original codebase is here: https://github.com/google-research/bert, and in this section we will
code up from scratch, the implementation of BERT-Tiny

## References
[1] Devlin, J., Chang, M., Lee, K., Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional
Transformers for Language Understanding. [arXiv:1810.04805](https://arxiv.org/abs/1810.04805).
