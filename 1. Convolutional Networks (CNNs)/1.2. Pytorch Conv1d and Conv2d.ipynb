{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2. Pytorch Conv1d and Conv2d\n",
    "**Goal:** Explain, and demo how to use the `nn.Conv1d` and `nn.Conv2d` modules.\n",
    "\n",
    "We will also explore the different hyperparamters, mainly\n",
    " - `stride`\n",
    " - `dilation`\n",
    " - `kernel_size`\n",
    "\n",
    "And explain what the following are\n",
    " - channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv1D and channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I create a convolution kernel $k_{1}$ with a kernel size of 5 and a 0 padding of 2 and convolve\n",
    "it a batch of 128 inputs, each of length 100.\n",
    "\n",
    "Note the following:\n",
    " - The input tensor is 3D. The first dimension is the batch size, the second is the 'number of channels'\n",
    " and the third dimension is the input length.\n",
    " - How the padding size is computed.\n",
    " - The weights are 3D. The shape of `weights` is `(1, 1, 5)` and not just `(5)`.\n",
    " - There is a bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 128, num. channels: 1, input_length: 100\n",
      "\n",
      "('weight', Parameter containing:\n",
      "tensor([[[0.0807, 0.3159, 0.2228, 0.2667, 0.0645]]], requires_grad=True))\n",
      "weights.shape: torch.Size([1, 1, 5])\n",
      "\n",
      "('bias', Parameter containing:\n",
      "tensor([0.2995], requires_grad=True))\n",
      "biases.shape: torch.Size([1])\n",
      "\n",
      "y1.shape torch.Size([128, 1, 100]). Identical shape to x1.\n"
     ]
    }
   ],
   "source": [
    "kernel_size = 5\n",
    "\n",
    "k1 = nn.Conv1d(\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    kernel_size=kernel_size,\n",
    "    padding=kernel_size//2,\n",
    ")\n",
    "\n",
    "x1 = torch.randn(128, 1, 100)\n",
    "\n",
    "print(f'Batch size: {x1.shape[0]}, num. channels: {x1.shape[1]}, input_length: {x1.shape[2]}\\n')\n",
    "\n",
    "weights, biases = k1.named_parameters()\n",
    "\n",
    "print(weights)\n",
    "print(f'weights.shape: {weights[1].shape}\\n')\n",
    "\n",
    "print(biases)\n",
    "print(f'biases.shape: {biases[1].shape}\\n')\n",
    "\n",
    "with torch.no_grad():\n",
    "    y1 = k1(x1)\n",
    "\n",
    "print(f'y1.shape {y1.shape}. Identical shape to x1.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is the padding computed? \n",
    " - Padding is computed using `kernel_size//2` as this ensure that there is enough cells containing\n",
    " 0 on either side of the input tensor.\n",
    " - You'll generally find that kernels have odd dimensions to facilitate this.\n",
    "\n",
    "What are channels?\n",
    " - Samples often have multiple channels.\n",
    " - An example is more easily given in 2D. A batch of images may have 3 channels (RGB) so a batch of\n",
    " 256x256 images may have shape `(128, 3, 256)`. A grayscale image would only need a single channel so\n",
    " it would have shape `(128, 1, 256)`.\n",
    " - In 1D, you may financial timeseries datasets that have a time and price. This would yield 2 channels,\n",
    " giving a shape such as `(128, 2, X)`.\n",
    "\n",
    "Why is the kernel shape `(1, 1, 5)`\n",
    " - The shape of the kernel in pytorch is `(out_channels, in_channels, kernel_size)`, as you can see\n",
    " in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights.shape: torch.Size([2, 3, 5])\n",
      "\n",
      "('bias', Parameter containing:\n",
      "tensor([ 0.1754, -0.2114], requires_grad=True))\n",
      "biases.shape: torch.Size([2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k2 = nn.Conv1d(\n",
    "    in_channels=3,\n",
    "    out_channels=2,\n",
    "    kernel_size=kernel_size,\n",
    "    padding=kernel_size//2,\n",
    ")\n",
    "\n",
    "weights, biases = k2.named_parameters()\n",
    "\n",
    "print(f'weights.shape: {weights[1].shape}\\n')\n",
    "\n",
    "print(biases)\n",
    "print(f'biases.shape: {biases[1].shape}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the bias?\n",
    " - Yes, there will be as many biases as there are `out_channels`, as you can see in the above twoo\n",
    " examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv2d\n",
    "You can create a 2D convolutional kernel in a very similar way to a 1D kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights.shape: torch.Size([2, 1, 3, 3])\n",
      "biases.shape: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "kernel_size = 3\n",
    "\n",
    "k3 = nn.Conv2d(\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    kernel_size=kernel_size,\n",
    "    padding=kernel_size//2,\n",
    ")\n",
    "\n",
    "weights, biases = k3.named_parameters()\n",
    "\n",
    "print(f'weights.shape: {weights[1].shape}')\n",
    "print(f'biases.shape: {biases[1].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the size of the kernel becomes `(out_channels, in_channels, kernel_size, kernel_size)`\n",
    "and the number of biases remains as 1 per `out_channel`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "The main hyperparameters for a convolutional layer are the `stride`, `dilation` and `kernel_size` (which\n",
    "has been sufficiently explored.)\n",
    "\n",
    "The stride is simply "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights.shape: torch.Size([2, 1, 3, 3])\n",
      "biases.shape: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "k4 = nn.Conv2d(\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    "    kernel_size=kernel_size,\n",
    "    padding=kernel_size//2,\n",
    ")\n",
    "\n",
    "weights, biases = k3.named_parameters()\n",
    "\n",
    "print(f'weights.shape: {weights[1].shape}')\n",
    "print(f'biases.shape: {biases[1].shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssl-cookbook-O0wafFVB-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
